{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fc6956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in ./.kenv/lib/python3.12/site-packages (from -r requirements.txt (line 1)) (0.23.0)\n",
      "Requirement already satisfied: torch in ./.kenv/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (2.9.1)\n",
      "Requirement already satisfied: torcheval in ./.kenv/lib/python3.12/site-packages (from -r requirements.txt (line 3)) (0.0.7)\n",
      "Requirement already satisfied: torchvision in ./.kenv/lib/python3.12/site-packages (from -r requirements.txt (line 4)) (0.24.1)\n",
      "Requirement already satisfied: torchaudio in ./.kenv/lib/python3.12/site-packages (from -r requirements.txt (line 5)) (2.9.1)\n",
      "Requirement already satisfied: numpy in ./.kenv/lib/python3.12/site-packages (from -r requirements.txt (line 6)) (2.3.5)\n",
      "Requirement already satisfied: scikit-learn in ./.kenv/lib/python3.12/site-packages (from -r requirements.txt (line 7)) (1.7.2)\n",
      "Requirement already satisfied: tensorflow in ./.kenv/lib/python3.12/site-packages (from -r requirements.txt (line 8)) (2.20.0)\n",
      "Requirement already satisfied: PyWavelets in ./.kenv/lib/python3.12/site-packages (from -r requirements.txt (line 9)) (1.9.0)\n",
      "Requirement already satisfied: scipy in ./.kenv/lib/python3.12/site-packages (from -r requirements.txt (line 10)) (1.16.3)\n",
      "Requirement already satisfied: spectrum in ./.kenv/lib/python3.12/site-packages (from -r requirements.txt (line 11)) (0.9.0)\n",
      "Requirement already satisfied: seaborn in ./.kenv/lib/python3.12/site-packages (from -r requirements.txt (line 12)) (0.13.2)\n",
      "Requirement already satisfied: tqdm in ./.kenv/lib/python3.12/site-packages (from -r requirements.txt (line 13)) (4.67.1)\n",
      "Requirement already satisfied: ucimlrepo in ./.kenv/lib/python3.12/site-packages (from -r requirements.txt (line 16)) (0.0.7)\n",
      "Requirement already satisfied: transformers in ./.kenv/lib/python3.12/site-packages (from -r requirements.txt (line 17)) (4.57.3)\n",
      "Requirement already satisfied: datasets in ./.kenv/lib/python3.12/site-packages (from -r requirements.txt (line 18)) (4.4.1)\n",
      "Requirement already satisfied: tf-keras in ./.kenv/lib/python3.12/site-packages (from -r requirements.txt (line 19)) (2.20.1)\n",
      "Requirement already satisfied: pytest in ./.kenv/lib/python3.12/site-packages (from -r requirements.txt (line 20)) (9.0.1)\n",
      "Requirement already satisfied: click>=8.0.1 in ./.kenv/lib/python3.12/site-packages (from wandb->-r requirements.txt (line 1)) (8.3.1)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in ./.kenv/lib/python3.12/site-packages (from wandb->-r requirements.txt (line 1)) (3.1.45)\n",
      "Requirement already satisfied: packaging in ./.kenv/lib/python3.12/site-packages (from wandb->-r requirements.txt (line 1)) (25.0)\n",
      "Requirement already satisfied: platformdirs in ./.kenv/lib/python3.12/site-packages (from wandb->-r requirements.txt (line 1)) (4.5.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in ./.kenv/lib/python3.12/site-packages (from wandb->-r requirements.txt (line 1)) (6.33.1)\n",
      "Requirement already satisfied: pydantic<3 in ./.kenv/lib/python3.12/site-packages (from wandb->-r requirements.txt (line 1)) (2.12.5)\n",
      "Requirement already satisfied: pyyaml in ./.kenv/lib/python3.12/site-packages (from wandb->-r requirements.txt (line 1)) (6.0.3)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in ./.kenv/lib/python3.12/site-packages (from wandb->-r requirements.txt (line 1)) (2.32.5)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in ./.kenv/lib/python3.12/site-packages (from wandb->-r requirements.txt (line 1)) (2.46.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.8 in ./.kenv/lib/python3.12/site-packages (from wandb->-r requirements.txt (line 1)) (4.15.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.kenv/lib/python3.12/site-packages (from pydantic<3->wandb->-r requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in ./.kenv/lib/python3.12/site-packages (from pydantic<3->wandb->-r requirements.txt (line 1)) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in ./.kenv/lib/python3.12/site-packages (from pydantic<3->wandb->-r requirements.txt (line 1)) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.kenv/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb->-r requirements.txt (line 1)) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.kenv/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb->-r requirements.txt (line 1)) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.kenv/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb->-r requirements.txt (line 1)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.kenv/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb->-r requirements.txt (line 1)) (2025.11.12)\n",
      "Requirement already satisfied: filelock in ./.kenv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (3.20.0)\n",
      "Requirement already satisfied: setuptools in ./.kenv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.kenv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./.kenv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (3.6)\n",
      "Requirement already satisfied: jinja2 in ./.kenv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in ./.kenv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 2)) (2025.10.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.kenv/lib/python3.12/site-packages (from torchvision->-r requirements.txt (line 4)) (12.0.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.kenv/lib/python3.12/site-packages (from scikit-learn->-r requirements.txt (line 7)) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.kenv/lib/python3.12/site-packages (from scikit-learn->-r requirements.txt (line 7)) (3.6.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in ./.kenv/lib/python3.12/site-packages (from tensorflow->-r requirements.txt (line 8)) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in ./.kenv/lib/python3.12/site-packages (from tensorflow->-r requirements.txt (line 8)) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in ./.kenv/lib/python3.12/site-packages (from tensorflow->-r requirements.txt (line 8)) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in ./.kenv/lib/python3.12/site-packages (from tensorflow->-r requirements.txt (line 8)) (0.6.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in ./.kenv/lib/python3.12/site-packages (from tensorflow->-r requirements.txt (line 8)) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in ./.kenv/lib/python3.12/site-packages (from tensorflow->-r requirements.txt (line 8)) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in ./.kenv/lib/python3.12/site-packages (from tensorflow->-r requirements.txt (line 8)) (3.4.0)\n",
      "Requirement already satisfied: six>=1.12.0 in ./.kenv/lib/python3.12/site-packages (from tensorflow->-r requirements.txt (line 8)) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./.kenv/lib/python3.12/site-packages (from tensorflow->-r requirements.txt (line 8)) (3.2.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in ./.kenv/lib/python3.12/site-packages (from tensorflow->-r requirements.txt (line 8)) (2.0.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./.kenv/lib/python3.12/site-packages (from tensorflow->-r requirements.txt (line 8)) (1.76.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in ./.kenv/lib/python3.12/site-packages (from tensorflow->-r requirements.txt (line 8)) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in ./.kenv/lib/python3.12/site-packages (from tensorflow->-r requirements.txt (line 8)) (3.12.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in ./.kenv/lib/python3.12/site-packages (from tensorflow->-r requirements.txt (line 8)) (3.15.1)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in ./.kenv/lib/python3.12/site-packages (from tensorflow->-r requirements.txt (line 8)) (0.5.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./.kenv/lib/python3.12/site-packages (from tensorboard~=2.20.0->tensorflow->-r requirements.txt (line 8)) (3.10)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./.kenv/lib/python3.12/site-packages (from tensorboard~=2.20.0->tensorflow->-r requirements.txt (line 8)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./.kenv/lib/python3.12/site-packages (from tensorboard~=2.20.0->tensorflow->-r requirements.txt (line 8)) (3.1.3)\n",
      "Requirement already satisfied: easydev in ./.kenv/lib/python3.12/site-packages (from spectrum->-r requirements.txt (line 11)) (0.13.3)\n",
      "Requirement already satisfied: matplotlib in ./.kenv/lib/python3.12/site-packages (from spectrum->-r requirements.txt (line 11)) (3.10.7)\n",
      "Requirement already satisfied: pandas>=1.2 in ./.kenv/lib/python3.12/site-packages (from seaborn->-r requirements.txt (line 12)) (2.3.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./.kenv/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 17)) (0.36.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.kenv/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 17)) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./.kenv/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 17)) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.kenv/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 17)) (0.7.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.kenv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers->-r requirements.txt (line 17)) (1.2.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in ./.kenv/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 18)) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in ./.kenv/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 18)) (0.4.0)\n",
      "Requirement already satisfied: httpx<1.0.0 in ./.kenv/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 18)) (0.28.1)\n",
      "Requirement already satisfied: xxhash in ./.kenv/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 18)) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in ./.kenv/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 18)) (0.70.18)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./.kenv/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 18)) (3.13.2)\n",
      "Requirement already satisfied: anyio in ./.kenv/lib/python3.12/site-packages (from httpx<1.0.0->datasets->-r requirements.txt (line 18)) (4.12.0)\n",
      "Requirement already satisfied: httpcore==1.* in ./.kenv/lib/python3.12/site-packages (from httpx<1.0.0->datasets->-r requirements.txt (line 18)) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./.kenv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets->-r requirements.txt (line 18)) (0.16.0)\n",
      "Requirement already satisfied: iniconfig>=1.0.1 in ./.kenv/lib/python3.12/site-packages (from pytest->-r requirements.txt (line 20)) (2.3.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in ./.kenv/lib/python3.12/site-packages (from pytest->-r requirements.txt (line 20)) (1.6.0)\n",
      "Requirement already satisfied: pygments>=2.7.2 in ./.kenv/lib/python3.12/site-packages (from pytest->-r requirements.txt (line 20)) (2.19.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.kenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 18)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./.kenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 18)) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.kenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 18)) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.kenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 18)) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.kenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 18)) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.kenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 18)) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.kenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 18)) (1.22.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./.kenv/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow->-r requirements.txt (line 8)) (0.45.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in ./.kenv/lib/python3.12/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 1)) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in ./.kenv/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 1)) (5.0.2)\n",
      "Requirement already satisfied: rich in ./.kenv/lib/python3.12/site-packages (from keras>=3.10.0->tensorflow->-r requirements.txt (line 8)) (14.2.0)\n",
      "Requirement already satisfied: namex in ./.kenv/lib/python3.12/site-packages (from keras>=3.10.0->tensorflow->-r requirements.txt (line 8)) (0.1.0)\n",
      "Requirement already satisfied: optree in ./.kenv/lib/python3.12/site-packages (from keras>=3.10.0->tensorflow->-r requirements.txt (line 8)) (0.18.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.kenv/lib/python3.12/site-packages (from matplotlib->spectrum->-r requirements.txt (line 11)) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.kenv/lib/python3.12/site-packages (from matplotlib->spectrum->-r requirements.txt (line 11)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.kenv/lib/python3.12/site-packages (from matplotlib->spectrum->-r requirements.txt (line 11)) (4.61.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.kenv/lib/python3.12/site-packages (from matplotlib->spectrum->-r requirements.txt (line 11)) (1.4.9)\n",
      "Requirement already satisfied: pyparsing>=3 in ./.kenv/lib/python3.12/site-packages (from matplotlib->spectrum->-r requirements.txt (line 11)) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.kenv/lib/python3.12/site-packages (from matplotlib->spectrum->-r requirements.txt (line 11)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.kenv/lib/python3.12/site-packages (from pandas>=1.2->seaborn->-r requirements.txt (line 12)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.kenv/lib/python3.12/site-packages (from pandas>=1.2->seaborn->-r requirements.txt (line 12)) (2025.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.kenv/lib/python3.12/site-packages (from sympy>=1.13.3->torch->-r requirements.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./.kenv/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow->-r requirements.txt (line 8)) (3.0.3)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.6 in ./.kenv/lib/python3.12/site-packages (from easydev->spectrum->-r requirements.txt (line 11)) (0.4.6)\n",
      "Requirement already satisfied: colorlog<7.0.0,>=6.8.2 in ./.kenv/lib/python3.12/site-packages (from easydev->spectrum->-r requirements.txt (line 11)) (6.10.1)\n",
      "Requirement already satisfied: line-profiler<5.0.0,>=4.1.2 in ./.kenv/lib/python3.12/site-packages (from easydev->spectrum->-r requirements.txt (line 11)) (4.2.0)\n",
      "Requirement already satisfied: pexpect<5.0.0,>=4.9.0 in ./.kenv/lib/python3.12/site-packages (from easydev->spectrum->-r requirements.txt (line 11)) (4.9.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./.kenv/lib/python3.12/site-packages (from pexpect<5.0.0,>=4.9.0->easydev->spectrum->-r requirements.txt (line 11)) (0.7.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.kenv/lib/python3.12/site-packages (from rich->keras>=3.10.0->tensorflow->-r requirements.txt (line 8)) (4.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.kenv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow->-r requirements.txt (line 8)) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72eea00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== LIBRARY IMPORTS =====\n",
    "# Import necessary libraries for EEG signal processing and machine learning\n",
    "\n",
    "# PyTorch for neural networks and deep learning\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Scikit-learn for data preprocessing and evaluation metrics\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# NumPy for numerical computations\n",
    "import numpy as np\n",
    "\n",
    "# SciPy for signal processing (Short-Time Fourier Transform)\n",
    "from scipy.signal import stft\n",
    "\n",
    "# Matplotlib for plotting and visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Custom modules for data loading and constants\n",
    "from loading_data import *\n",
    "from CONSTANT import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f957c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== HELPER FUNCTIONS =====\n",
    "# This section contains utility functions for EEG signal processing\n",
    "\n",
    "def number_fft(window_size):\n",
    "    \"\"\"\n",
    "    Calculate the next power of 2 greater than window_size-1.\n",
    "    This is used for optimal FFT computation.\n",
    "    \n",
    "    Args:\n",
    "        window_size (int): Size of the analysis window\n",
    "        \n",
    "    Returns:\n",
    "        int: Next power of 2 (2^power)\n",
    "    \"\"\"\n",
    "    power = 0\n",
    "    window_tmp = window_size - 1\n",
    "    while(window_tmp != 1):\n",
    "        power = power + 1\n",
    "        window_tmp = int(window_tmp / 2)\n",
    "    return pow(2, power + 1)\n",
    "\n",
    "def feature_extraction(input_data, stft_parameters=DEFAULT_STFT_PARAMETERS, label_num=0, fs=SAMPLING_FQ):\n",
    "    \"\"\"\n",
    "    Extract features from EEG data using Short-Time Fourier Transform (STFT).\n",
    "    \n",
    "    This function processes multi-channel EEG data and extracts spectral features\n",
    "    for emotion classification. It applies STFT, computes power spectrum,\n",
    "    and performs smoothing and normalization.\n",
    "    \n",
    "    Args:\n",
    "        input_data (np.ndarray): Raw EEG data of shape (samples, channels)\n",
    "        stft_parameters (dict): Parameters for STFT computation\n",
    "        label_num (int): Class label for the data (0=focused, 1=unfocused, 2=drowsed)\n",
    "        fs (int): Sampling frequency\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (features, labels) where features is a transposed array and labels is a list\n",
    "    \"\"\"\n",
    "    \n",
    "    def square(x): return (np.abs(x))**2\n",
    "    def decibels(x): return 10*np.log10(x)\n",
    "    \n",
    "    # Extract STFT parameters\n",
    "    window_size = stft_parameters['window_size']\n",
    "    window_shift = stft_parameters['window_shift']\n",
    "    avg_window_size = stft_parameters['avg_filter_size']\n",
    "    window_type = stft_parameters['window_type']\n",
    "\n",
    "    times = fs / window_shift\n",
    "    feature_list = []\n",
    "\n",
    "    # Calculate optimal FFT size (power of 2)\n",
    "    nfft_size = number_fft(window_size)\n",
    "\n",
    "    # Process each EEG channel (62 channels total)\n",
    "    for i in range(62):\n",
    "        channel_feature_list = []\n",
    "        \n",
    "        # Apply STFT to extract time-frequency features\n",
    "        # STFT transforms time-domain signal to time-frequency domain\n",
    "        eeg_feq = stft(input_data[:, i], fs, window_type, nperseg=window_size*fs,\n",
    "                       noverlap=window_size*fs-window_shift, nfft=nfft_size*fs)\n",
    "        \n",
    "        # Extract the power spectrum (magnitude squared)\n",
    "        eeg_feq_data = eeg_feq[-1]\n",
    "        eeg_feq_data = eeg_feq_data[0:-1, 0:-1]  # Remove DC and Nyquist components\n",
    "        eeg_feq_data = eeg_feq_data.reshape(128, int(nfft_size/2), -1)\n",
    "\n",
    "        # Process frequency bands (36 bands from 4Hz to 40Hz)\n",
    "        for j in range(36):\n",
    "            # Extract features from each frequency band (starting from 4Hz)\n",
    "            current = eeg_feq_data[j+1, :, :].mean(axis=0)\n",
    "            \n",
    "            # Convert to power spectrum\n",
    "            current = np.apply_along_axis(square, axis=0, arr=current)\n",
    "            \n",
    "            # Convert to decibels (log scale)\n",
    "            current = np.apply_along_axis(decibels, axis=0, arr=current)\n",
    "            \n",
    "            # Apply moving average smoothing\n",
    "            feature = moving_average_smooth(current, avg_window_size)\n",
    "            channel_feature_list.append(feature)\n",
    "            \n",
    "        # Standardize features across time windows\n",
    "        channel_feature_list = standardscaler_dataframe_train(\n",
    "            np.array(channel_feature_list))\n",
    "        \n",
    "        # Combine features from all channels\n",
    "        if (i == 0):\n",
    "            feature_list = np.array(channel_feature_list)\n",
    "        else:\n",
    "            feature_list = np.vstack(\n",
    "                (feature_list, np.array(channel_feature_list)))\n",
    "            \n",
    "    # Create label for the entire trial\n",
    "    # focused: 0 -> unfocused: 1 -> drowsed: 2\n",
    "    label = [label_num] * feature_list.shape[1]\n",
    "    \n",
    "    # Return transposed features (time windows as samples)\n",
    "    return feature_list.transpose(), label\n",
    "\n",
    "def moving_average_smooth(interval, window_size):\n",
    "    \"\"\"\n",
    "    Apply moving average smoothing to reduce noise in the signal.\n",
    "    \n",
    "    Args:\n",
    "        interval (np.ndarray): Input signal\n",
    "        window_size (int): Size of the averaging window\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Smoothed signal\n",
    "    \"\"\"\n",
    "    window = np.ones(int(window_size)) / float(window_size)\n",
    "    re = np.convolve(interval, window, 'same')\n",
    "    return re\n",
    "\n",
    "def standardscaler_dataframe_train(feature_list):\n",
    "    \"\"\"\n",
    "    Standardize features using StandardScaler (zero mean, unit variance).\n",
    "    \n",
    "    This function normalizes each feature independently to improve\n",
    "    model training and convergence.\n",
    "    \n",
    "    Args:\n",
    "        feature_list (list): List of feature arrays\n",
    "        \n",
    "    Returns:\n",
    "        list: List of standardized feature arrays\n",
    "    \"\"\"\n",
    "    scaler_list = list()\n",
    "    new_feature_list = list()\n",
    "    \n",
    "    # Standardize each feature independently\n",
    "    for i in range(len(feature_list)):\n",
    "        scaler = StandardScaler()\n",
    "        x = np.array(feature_list[i]).reshape(-1, 1)\n",
    "        x = scaler.fit_transform(x)\n",
    "        new_feature_list.append(x.reshape(-1))\n",
    "        scaler_list.append(scaler)\n",
    "\n",
    "    return new_feature_list\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"\n",
    "    Set random seeds for reproducibility.\n",
    "    \n",
    "    Ensures consistent results across different runs by fixing\n",
    "    random number generation in Python libraries.\n",
    "    \n",
    "    Args:\n",
    "        seed (int): Random seed value\n",
    "    \"\"\"\n",
    "    random.seed(seed)  # python\n",
    "    np.random.seed(seed)  # numpy\n",
    "    # PyTorch seeds commented out as they're not needed for current implementation\n",
    "    # torch.manual_seed(seed) # pytorch\n",
    "    # torch.cuda.manual_seed(seed)\n",
    "    # torch.backends.cudnn.deterministic = True\n",
    "    # torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set seed for reproducible results\n",
    "set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d676df89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== DATA PREPARATION FOR CROSS-VALIDATION =====\n",
    "# This function prepares data for leave-one-session-out cross-validation\n",
    "\n",
    "def process_dataset_to_fold(label_choices=[0,3]):\n",
    "    \"\"\"\n",
    "    Process dataset for leave-one-session-out cross-validation.\n",
    "    \n",
    "    This function creates training and test sets by leaving out each session\n",
    "    in turn. This is important for evaluating model generalization across\n",
    "    different recording sessions.\n",
    "    \n",
    "    Args:\n",
    "        label_choices (list): List of emotion labels to include\n",
    "                             [0,3] typically means focused vs drowsed\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (X_train_folds, X_test_folds, y_train_folds, y_test_folds)\n",
    "               Each list contains 3 elements (one for each fold)\n",
    "    \"\"\"\n",
    "    X_train_folds = []\n",
    "    X_test_folds = []\n",
    "    y_train_folds = []\n",
    "    y_test_folds = []\n",
    "\n",
    "    # Perform leave-one-session-out CV (3 sessions total)\n",
    "    for session_except in [1,2,3]:\n",
    "        X_train_mean = []\n",
    "        X_train = []\n",
    "        y_train = []\n",
    "        \n",
    "        # Use remaining sessions for training\n",
    "        list_session = [1,2,3]\n",
    "        list_session.remove(session_except)\n",
    "        \n",
    "        # Process all files from training sessions\n",
    "        for session_num in list_session:\n",
    "            for file_num in range(24):\n",
    "                # Get labels for current session\n",
    "                label_list = SESSION_LABELS[str(session_num)]\n",
    "                \n",
    "                # Extract features from EEG data\n",
    "                x_part, y_part = feature_extraction(\n",
    "                    input_data=d[str(session_num)][str(file_num)], \n",
    "                    label_num=label_list[file_num]\n",
    "                )\n",
    "                \n",
    "                # Only include specified emotion classes\n",
    "                if label_list[file_num] in label_choices:\n",
    "                    X_train_mean.extend(list(x_part))\n",
    "                    X_train.extend(list(x_part))\n",
    "                    y_train.extend(list(y_part))\n",
    "\n",
    "        # Process test session\n",
    "        X_test_mean = []\n",
    "        X_test = []\n",
    "        y_test = []\n",
    "        \n",
    "        for file_num in range(24):\n",
    "            label_list = SESSION_LABELS[str(session_except)]\n",
    "            x_part, y_part = feature_extraction(\n",
    "                input_data=d[str(session_except)][str(file_num)], \n",
    "                label_num=label_list[file_num]\n",
    "            )\n",
    "            if label_list[file_num] in label_choices:\n",
    "                X_test_mean.extend(list(x_part))\n",
    "                X_test.extend(list(x_part))\n",
    "                y_test.extend(list(y_part))\n",
    "\n",
    "        # Append fold data\n",
    "        X_train_folds.append(X_train)\n",
    "        X_test_folds.append(X_test)\n",
    "        y_train_folds.append(y_train)\n",
    "        y_test_folds.append(y_test)\n",
    "        \n",
    "    return X_train_folds, X_test_folds, y_train_folds, y_test_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf5939c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom PyTorch Dataset Classes\n",
    "\n",
    "class MyNNDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset for Neural Network (MLP) model.\n",
    "    \n",
    "    This dataset flattens the 2D feature matrix (62 channels × 36 frequency bands)\n",
    "    into a 1D vector (2232 features) for input to a standard neural network.\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y):\n",
    "        self.X = []\n",
    "        for x in X:\n",
    "            # Convert to tensor and flatten to 1D\n",
    "            x = torch.tensor(x, dtype=torch.float32)\n",
    "            x = x.flatten()          # Convert 62×36 matrix to 2232-dim vector\n",
    "            self.X.append(x)\n",
    "        self.X = torch.stack(self.X)  # Stack into tensor of shape (N, 2232)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "class MyCNNDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset for Convolutional Neural Network model.\n",
    "    \n",
    "    This dataset preserves the 2D structure of the EEG features\n",
    "    as a spatial representation (62 channels × 36 frequency bands)\n",
    "    which can be treated like an image for CNN processing.\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y):\n",
    "        self.X = []\n",
    "        for x in X:\n",
    "            x = torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "            # Ensure correct shape for CNN\n",
    "            # If it's a vector (2232,), reshape to (62, 36)\n",
    "            if x.ndim == 1 and x.numel() == 2232:\n",
    "                x = x.reshape(62, 36)\n",
    "\n",
    "            # If it's already (62, 36), add channel dimension\n",
    "            if x.shape == (62, 36):\n",
    "                x = x.unsqueeze(0)  # Add channel dim: (1, 62, 36)\n",
    "            self.X.append(x)\n",
    "\n",
    "        self.X = torch.stack(self.X)   # Shape: (N, 1, 62, 36)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0e55a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Model Architectures\n",
    "\n",
    "class NNClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Layer Perceptron (MLP) for EEG emotion classification.\n",
    "    \n",
    "    This is a standard fully-connected neural network that takes\n",
    "    flattened EEG features (2232 dimensions) as input.\n",
    "    \n",
    "    Architecture:\n",
    "    - Input: 2232 features (62 channels × 36 frequency bands)\n",
    "    - Hidden layers: 512 → 128 → 32 units\n",
    "    - Output: num_classes units (emotion categories)\n",
    "    - Activation: ReLU for hidden layers\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, num_classes=4):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),  # First hidden layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),        # Second hidden layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 32),         # Third hidden layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, num_classes)  # Output layer (no activation)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class CNNClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional Neural Network for EEG emotion classification.\n",
    "    \n",
    "    This CNN treats the EEG feature matrix (62×36) as a 2D spatial map,\n",
    "    applying convolution operations to learn spatial patterns.\n",
    "    \n",
    "    Architecture:\n",
    "    - Input: 1×62×36 tensor (1 channel, 62 electrodes, 36 frequency bands)\n",
    "    - Conv layers: 16→32 filters with 3×3 kernels\n",
    "    - Max pooling: 2×2 windows\n",
    "    - Fully connected: 32×9×15 → 128 → num_classes\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=4):\n",
    "        super().__init__()\n",
    "        # Convolutional layers for feature extraction\n",
    "        self.conv = nn.Sequential(\n",
    "            # First conv block\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),   # Output: (16, 31, 18)\n",
    "\n",
    "            # Second conv block\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),   # Output: (32, 15, 9)\n",
    "        )\n",
    "\n",
    "        # Fully connected layers for classification\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(32*15*9, 128),  # Note: Check input size based on actual conv output\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolutional layers\n",
    "        out = self.conv(x)\n",
    "        # Flatten for fully connected layers\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        # Apply fully connected layers\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c85261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Evaluation Functions\n",
    "\n",
    "def train_and_test(\n",
    "    X_train, y_train, X_test, y_test, \n",
    "    epochs=20, batch_size=16, lr=1e-3,\n",
    "    device=device,\n",
    "    model_type='nn', print_check=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Train and evaluate either NN or CNN model on EEG data.\n",
    "    \n",
    "    This function handles the complete training pipeline including:\n",
    "    - Dataset creation and DataLoader setup\n",
    "    - Model initialization\n",
    "    - Training loop\n",
    "    - Evaluation with confusion matrix and classification report\n",
    "    \n",
    "    Args:\n",
    "        X_train, y_train: Training data and labels\n",
    "        X_test, y_test: Test data and labels\n",
    "        epochs: Number of training epochs\n",
    "        batch_size: Batch size for training\n",
    "        lr: Learning rate\n",
    "        device: PyTorch device (cpu/cuda)\n",
    "        model_type: 'nn' for MLP, 'cnn' for CNN\n",
    "        print_check: Whether to print detailed results\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (trained_model, test_accuracy)\n",
    "    \"\"\"\n",
    "\n",
    "    # ===== DATA PREPARATION =====\n",
    "    # Create appropriate datasets based on model type\n",
    "    if model_type.lower() == 'nn':\n",
    "        train_ds = MyNNDataset(X_train, y_train)\n",
    "        test_ds  = MyNNDataset(X_test,  y_test)\n",
    "        model = NNClassifier(input_dim=len(X_train[1])).to(device)\n",
    "    elif model_type.lower() == 'cnn':\n",
    "        train_ds = MyCNNDataset(X_train, y_train)\n",
    "        test_ds  = MyCNNDataset(X_test,  y_test)\n",
    "        model = CNNClassifier().to(device)\n",
    "    else:\n",
    "        raise TypeError(\"Only allowed cnn or nn model. Check your option and run again!\")\n",
    "\n",
    "    # Create DataLoaders for batch processing\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # ===== TRAINING SETUP =====\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()  # Multi-class classification loss\n",
    "\n",
    "    # ===== TRAINING LOOP =====\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        total_loss = 0\n",
    "        \n",
    "        # Iterate over batches\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(batch_X)\n",
    "            loss = loss_fn(logits, batch_y)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "\n",
    "    # ===== EVALUATION =====\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Disable gradient computation for evaluation\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            \n",
    "            # Get predictions\n",
    "            logits = model(batch_X)\n",
    "            pred = logits.argmax(dim=1)\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            correct += (pred == batch_y).sum().item()\n",
    "            total += batch_y.size(0)\n",
    "\n",
    "            # Store predictions for detailed metrics\n",
    "            all_preds.extend(pred.cpu().numpy())\n",
    "            all_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "    # Calculate final accuracy\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    \n",
    "    # ===== RESULTS VISUALIZATION =====\n",
    "    if print_check:\n",
    "        # Determine model name for display\n",
    "        if model_type == 'nn':\n",
    "            print('Neural Network training:')\n",
    "            model_name = 'Neural Network'\n",
    "        elif model_type == 'cnn':\n",
    "            print('\\nConvolutional Neural Network training:')\n",
    "            model_name = 'Convolutional Neural Network'\n",
    "        \n",
    "        # Create and display confusion matrix\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, \n",
    "                                     display_labels=['Class 0', 'Class 1'])\n",
    "        disp.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"Confusion Matrix {model_name}\")\n",
    "        plt.show()\n",
    "        \n",
    "        # Print performance metrics\n",
    "        print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "        print(\"\\nClassification report:\")\n",
    "        print(classification_report(all_labels, all_preds))\n",
    "\n",
    "    return model, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9063b04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== EXPERIMENT EXECUTION =====\n",
    "# This cell runs the complete cross-validation experiment\n",
    "\n",
    "# Prepare cross-validation datasets for focused vs drowsed classification\n",
    "# Label choices: [0=focused, 3=drowsed]\n",
    "X_train_folds, X_test_folds, y_train_folds, y_test_folds = process_dataset_to_fold(label_choices=[0,3])\n",
    "\n",
    "# Initialize lists to store accuracy results\n",
    "acc_NN_summarize = []  # Neural Network accuracies\n",
    "acc_CNN_summarize = [] # CNN accuracies\n",
    "\n",
    "# Flag to control output printing\n",
    "PRINT_CHECK = True\n",
    "\n",
    "# ===== CROSS-VALIDATION LOOP =====\n",
    "# Perform 3-fold cross-validation (leave one session out each time)\n",
    "for i in range(3):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"FOLD {i+1}/3 - Session {i+1} as test set\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Train and evaluate Neural Network\n",
    "    print(\"\\nTraining Neural Network...\")\n",
    "    modelNN, accNN = train_and_test(\n",
    "        X_train=X_train_folds[i], \n",
    "        y_train=y_train_folds[i], \n",
    "        X_test=X_test_folds[i], \n",
    "        y_test=y_test_folds[i], \n",
    "        epochs=20, \n",
    "        batch_size=64, \n",
    "        lr=1e-3, \n",
    "        model_type='nn', \n",
    "        print_check=PRINT_CHECK\n",
    "    )\n",
    "    acc_NN_summarize.append(accNN)\n",
    "    print(f\"Neural Network Accuracy (Fold {i+1}): {accNN*100:.2f}%\")\n",
    "    \n",
    "    # Train and evaluate CNN\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Training Convolutional Neural Network...\")\n",
    "    modelCNN, accCNN = train_and_test(\n",
    "        X_train=X_train_folds[i], \n",
    "        y_train=y_train_folds[i], \n",
    "        X_test=X_test_folds[i], \n",
    "        y_test=y_test_folds[i], \n",
    "        epochs=20, \n",
    "        batch_size=64, \n",
    "        lr=1e-3, \n",
    "        model_type='cnn', \n",
    "        print_check=PRINT_CHECK\n",
    "    )\n",
    "    acc_CNN_summarize.append(accCNN)\n",
    "    print(f\"CNN Accuracy (Fold {i+1}): {accCNN*100:.2f}%\")\n",
    "\n",
    "# ===== FINAL RESULTS SUMMARY =====\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "# Calculate and display mean accuracies across folds\n",
    "nn_mean_acc = np.mean(np.array(acc_NN_summarize))\n",
    "cnn_mean_acc = np.mean(np.array(acc_CNN_summarize))\n",
    "\n",
    "print(f\"\\nNeural Network Performance:\")\n",
    "print(f\"  - Fold accuracies: {[f'{acc*100:.2f}%' for acc in acc_NN_summarize]}\")\n",
    "print(f\"  - Mean accuracy: {nn_mean_acc*100:.2f}%\")\n",
    "print(f\"  - Std deviation: {np.std(np.array(acc_NN_summarize))*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nConvolutional Neural Network Performance:\")\n",
    "print(f\"  - Fold accuracies: {[f'{acc*100:.2f}%' for acc in acc_CNN_summarize]}\")\n",
    "print(f\"  - Mean accuracy: {cnn_mean_acc*100:.2f}%\")\n",
    "print(f\"  - Std deviation: {np.std(np.array(acc_CNN_summarize))*100:.2f}%\")\n",
    "\n",
    "# Determine which model performed better\n",
    "if cnn_mean_acc > nn_mean_acc:\n",
    "    improvement = ((cnn_mean_acc - nn_mean_acc) / nn_mean_acc) * 100\n",
    "    print(f\"\\nCNN outperforms NN by {improvement:.2f}%\")\n",
    "elif nn_mean_acc > cnn_mean_acc:\n",
    "    improvement = ((nn_mean_acc - cnn_mean_acc) / cnn_mean_acc) * 100\n",
    "    print(f\"\\nNN outperforms CNN by {improvement:.2f}%\")\n",
    "else:\n",
    "    print(f\"\\nBoth models have similar performance\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".kenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
